{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPMe25cMxNgkGtQuHfHL2xA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f623790c790a4049a47eed0326f53017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0614255379fa49159ac5ded90437b137",
              "IPY_MODEL_c7d57835a51e4cb88d43ee93d0943c5a",
              "IPY_MODEL_5a45ed5433924cbdb07225435af0e312",
              "IPY_MODEL_150aa92ece3a436791164a9df5742594"
            ],
            "layout": "IPY_MODEL_5105d20d76d24a409e8454899ab9d1b5"
          }
        },
        "c15113fd98a741a5a00ec102b7a209b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13bf1ed303904d338a60208c372bd75a",
            "placeholder": "​",
            "style": "IPY_MODEL_6c9fd03b7e8c4437b451e4f83eeeadf3",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "189b3a6fd39c4b339ab981412cdc6728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_280b8932e7da4c75b665145ea557f56a",
            "placeholder": "​",
            "style": "IPY_MODEL_d95e0a5533ac4dd283fe1bfafde53136",
            "value": ""
          }
        },
        "75e7fbf728f44babb48bfd6085a7871e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1d9a68eb7f2341f7895bc722c8790e45",
            "style": "IPY_MODEL_89a4b0bd2c9b4d8ea443d2b7566645dd",
            "value": true
          }
        },
        "1a6b5bff459a4968966a08e88ccfee3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_82d9d8d997724cdd803487e10205f847",
            "style": "IPY_MODEL_58493e0814cf48d0afdf96a82d0133e6",
            "tooltip": ""
          }
        },
        "f7e35898035c4d98ac9de8e4d8b65b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969c521253a847468293ca594a35c500",
            "placeholder": "​",
            "style": "IPY_MODEL_30b1df499d3442b2b97a908894b0af3d",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "5105d20d76d24a409e8454899ab9d1b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "13bf1ed303904d338a60208c372bd75a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9fd03b7e8c4437b451e4f83eeeadf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "280b8932e7da4c75b665145ea557f56a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95e0a5533ac4dd283fe1bfafde53136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d9a68eb7f2341f7895bc722c8790e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89a4b0bd2c9b4d8ea443d2b7566645dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82d9d8d997724cdd803487e10205f847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58493e0814cf48d0afdf96a82d0133e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "969c521253a847468293ca594a35c500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30b1df499d3442b2b97a908894b0af3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81f1694d26f44d8d96d82f7949d912ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_338b66b17b8f4793b06f266ff31f7cd5",
            "placeholder": "​",
            "style": "IPY_MODEL_9ed75bed6bce4e7b976ac39e91d93141",
            "value": "Connecting..."
          }
        },
        "338b66b17b8f4793b06f266ff31f7cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ed75bed6bce4e7b976ac39e91d93141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0614255379fa49159ac5ded90437b137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92bd68aaf4f64ca1ae046d64d020879c",
            "placeholder": "​",
            "style": "IPY_MODEL_511274491d234b379783f05f70c0bdd8",
            "value": "Token is valid (permission: write)."
          }
        },
        "c7d57835a51e4cb88d43ee93d0943c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bad4db9e9c0405e94291e2bb20094d1",
            "placeholder": "​",
            "style": "IPY_MODEL_0df7b837a0544ab696855cb093324872",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "5a45ed5433924cbdb07225435af0e312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8728f858dc6847729cf536add0e5a628",
            "placeholder": "​",
            "style": "IPY_MODEL_200651f8c7ff450b8aa9051ece8fd238",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "150aa92ece3a436791164a9df5742594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_499f7147aca544c4996a5ffa01e72ac6",
            "placeholder": "​",
            "style": "IPY_MODEL_65621765fbc14d73a522f7b6a3365ee4",
            "value": "Login successful"
          }
        },
        "92bd68aaf4f64ca1ae046d64d020879c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511274491d234b379783f05f70c0bdd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bad4db9e9c0405e94291e2bb20094d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df7b837a0544ab696855cb093324872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8728f858dc6847729cf536add0e5a628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "200651f8c7ff450b8aa9051ece8fd238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "499f7147aca544c4996a5ffa01e72ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65621765fbc14d73a522f7b6a3365ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc71f1673e0e41e894c1dc67cbb0d793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e7c02da13de4c4f9f70279fad462a56",
              "IPY_MODEL_f2237504735e4ffe98dbeec509cb951e",
              "IPY_MODEL_bdba23601fd34fbebd5174e6c6050025"
            ],
            "layout": "IPY_MODEL_5fb5d7e968064273a9bf027001942bb0"
          }
        },
        "2e7c02da13de4c4f9f70279fad462a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928b699cb54a46b1a96db5faeb33f5fe",
            "placeholder": "​",
            "style": "IPY_MODEL_473b8a657eeb4eb0b3f651ddb34d506d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f2237504735e4ffe98dbeec509cb951e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd449b5b928e4f9aae06f4a7de5e1aa6",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_585574ed98e343e58256409e124e9b98",
            "value": 10
          }
        },
        "bdba23601fd34fbebd5174e6c6050025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ee124d26a0842f9b350df2fd4e0d0be",
            "placeholder": "​",
            "style": "IPY_MODEL_3a3f5abecb514056ae06d1f7d09a0a28",
            "value": " 10/10 [02:30&lt;00:00, 12.19s/it]"
          }
        },
        "5fb5d7e968064273a9bf027001942bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "928b699cb54a46b1a96db5faeb33f5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473b8a657eeb4eb0b3f651ddb34d506d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd449b5b928e4f9aae06f4a7de5e1aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "585574ed98e343e58256409e124e9b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ee124d26a0842f9b350df2fd4e0d0be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3f5abecb514056ae06d1f7d09a0a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d70a3648d7894639ba0b6e170ff06abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4de339982b58421b93010d61a1054adc",
              "IPY_MODEL_009a3863cc954f9aa98958a73b741933",
              "IPY_MODEL_09e4abecf32b4ef28b8098f857061832"
            ],
            "layout": "IPY_MODEL_6ccba1417f2c4b6588e3057cecfb0524"
          }
        },
        "4de339982b58421b93010d61a1054adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a781ad2ca940494d8112e51b0316fd6f",
            "placeholder": "​",
            "style": "IPY_MODEL_caa87935a39a45f4a9b419dd7225c14f",
            "value": "Map: 100%"
          }
        },
        "009a3863cc954f9aa98958a73b741933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32078580a2bb49589a2047848ff35720",
            "max": 121959,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_663c979859fb4c1ca34a3b42657a75ce",
            "value": 121959
          }
        },
        "09e4abecf32b4ef28b8098f857061832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b10ca489f66647bb993f9f7c9da34acd",
            "placeholder": "​",
            "style": "IPY_MODEL_ef1669d467b243118ae2f9d3b937e392",
            "value": " 121959/121959 [00:33&lt;00:00, 3821.44 examples/s]"
          }
        },
        "6ccba1417f2c4b6588e3057cecfb0524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a781ad2ca940494d8112e51b0316fd6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caa87935a39a45f4a9b419dd7225c14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32078580a2bb49589a2047848ff35720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663c979859fb4c1ca34a3b42657a75ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b10ca489f66647bb993f9f7c9da34acd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1669d467b243118ae2f9d3b937e392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dcf35c032c646a2a6159994d33bf49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d085de7c054540b5a676636a00c30053",
              "IPY_MODEL_07f7bb6cb2f34118a3d7505a53fd30f0",
              "IPY_MODEL_276f8da8d2a84e8c8cae348bd17cf721"
            ],
            "layout": "IPY_MODEL_f303331bb02947dd890abf85610b8ce2"
          }
        },
        "d085de7c054540b5a676636a00c30053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc4b16b362a14d03bf8737a22f091798",
            "placeholder": "​",
            "style": "IPY_MODEL_59ad509c976a46e190e8ce8bb0e0f4cf",
            "value": "adapter_model.bin: 100%"
          }
        },
        "07f7bb6cb2f34118a3d7505a53fd30f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b119483431a0469194bbde5d41fcba5d",
            "max": 134263757,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42fd3f5d88f84de987e03455091da066",
            "value": 134263757
          }
        },
        "276f8da8d2a84e8c8cae348bd17cf721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b78952b368474eec878a0eb6ba8f2e59",
            "placeholder": "​",
            "style": "IPY_MODEL_9effa02eda9b4a5bbeb600b83d02561c",
            "value": " 134M/134M [00:02&lt;00:00, 68.1MB/s]"
          }
        },
        "f303331bb02947dd890abf85610b8ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc4b16b362a14d03bf8737a22f091798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59ad509c976a46e190e8ce8bb0e0f4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b119483431a0469194bbde5d41fcba5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42fd3f5d88f84de987e03455091da066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b78952b368474eec878a0eb6ba8f2e59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9effa02eda9b4a5bbeb600b83d02561c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/HuggingFace/blob/main/amr_llama2_code_instr_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama2\n",
        "LLaMA 2 is available in 3 sizes:\n",
        "- 7b 7 billion parameters\n",
        "- 13b 13 billion parameters\n",
        "- 70b 70 billion parameters\n",
        "\n",
        "This notebook uses the 7b parameters to fine tune our dataset [TokenBender/code_instructions_120k_alpaca_style](https://huggingface.co/datasets/TokenBender/code_instructions_120k_alpaca_style)\n",
        "\n",
        "[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) is a language model fine-tuned using supervised learning from a LLaMA 7B model on 52K instruction-following demonstrations generated from OpenAI's text-davinci-003.\n"
      ],
      "metadata": {
        "id": "c9sTNPqhsnf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning a LLM\n",
        "**Why doing it?** To add a domain specific corpus of data to a foundational LLM, i.e. legal or medical corpus for example.\n",
        "\n",
        "Here are the general steps involved in fine-tuning a LLM:\n",
        "\n",
        "**Obtain a Pre-trained Model**: The first step is to have a pre-trained model ready for fine-tuning. Pre-training involves training the model on a large corpus of text data. The aim of this step is to learn a good representation of the language that can be used as a starting point for various tasks. OpenAI provides pre-trained models like GPT-3.\n",
        "\n",
        "**Prepare Your Fine-Tuning Data**: The next step is to gather and prepare the data you will use for fine-tuning. This data should be relevant to the specific task or domain you want the model to perform or understand better. For example, if you're fine-tuning the model for medical text generation, you might use a corpus of medical literature. The data should be preprocessed and formatted in a way that's compatible with the model.\n",
        "\n",
        "**Fine-Tune the Model**: Once you have your data prepared, you can start the fine-tuning process. This involves continuing the training of the model on your specific data. The learning rate during fine-tuning is usually much smaller than during pre-training because you don't want to drastically change the already learned representations. You're just trying to adapt them to your specific task.\n",
        "\n",
        "**Evaluate the Model**: After fine-tuning, it's important to evaluate the model on a separate validation dataset to ensure it's learning the correct patterns. This can be done by using metrics relevant to your specific task, like accuracy, F1-score, perplexity, etc.\n",
        "\n",
        "**Use the Fine-Tuned Model**: If the evaluation results are satisfactory, you can then use your fine-tuned model for your specific task. If not, you might need to go back and adjust some parameters, get more fine-tuning data, or make other changes.\n",
        "\n",
        "> Remember, fine-tuning a model effectively requires a good understanding of the model architecture, the task at hand, and the data you're working with. It's part art, part science."
      ],
      "metadata": {
        "id": "NeDbv2Refqpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient methods used to train a LLM\n",
        "\n",
        "- **LoRA**, which stands for Low-Rank Adapters (LoRA), are **small sets of trainable parameters, injected into each layer of the Transformer architecture** while fine-tuning. While original model weights are frozen and not updated, these small sets of injected weights are updated during fine-tuning. This greatly reduces the number of trainable parameters for downstream tasks. Gradients during stochastic gradient descent are passed through the frozen pre-trained model weights to the adapter. Thus, only these adapters, with a small memory footprint, are updated during the time of training.\n",
        "- **Quantization** means “rounding” off values, from one data type to another. It works with **squeezing larger values into data types with less number of bits**, but with a small loss of precision.\n"
      ],
      "metadata": {
        "id": "SQJ2p1cy2NFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Support for fine-tuning\n",
        "\n",
        "HuggingFace has released several libraries that can be used to fine-tune LLMs easily.\n",
        "\n",
        "These include:\n",
        "\n",
        "- **PEFT** (Parameter Efficient Fine Tuning) which has support for LoRA.\n",
        "- **Quantization** Support — Many models can be loaded in 8-bit and 4-bit precision using `bitsandbytes` module. The basic way to load a model in 4bit is to pass the argument `load_in_4bit=True` when calling the from_pretrained method.\n",
        "- **Accelerate** library — Accelerate library has many features to make it easy to reduce the memory requirements of models.\n",
        "- **SFT** (Supervised Fine-Tuning Trainer) — The SFT trainer is the trainer class for supervised fine-tuning of Large LLMs.\n",
        "\n",
        "These combined techniques are used here to train the llama2-7b on a code instruction dataset. Notice, we set the storage type to 4-bit and the computation type to FP-16."
      ],
      "metadata": {
        "id": "gGcyZXVR3eDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal Code\n",
        "\n",
        "Here, a 4-bit quantization and PEFT to fine-tune Llama2-7b on a single Google Colab instance!\n",
        "> Two major components that democratize the training of LLMs are: **Parameter-Efficient Fine-tuning (PEFT)** (e.g: LoRA (Low Rank Adapter)\n",
        ", Adapter) and **quantization** techniques (8-bit, 4-bit)\n",
        "\n",
        "\n",
        "![image](https://pbs.twimg.com/media/F1fj-SqWYAELWpM?format=jpg&name=medium)"
      ],
      "metadata": {
        "id": "gQWCryPcs8wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB:\n",
        "- `AutoModelForCausalLM` is used for auto-regressive language models like all the GPT models.\n",
        "- `SFTTrainer`: [Supervised fine-tuning](https://huggingface.co/docs/trl/main/en/sft_trainer)\n",
        "( (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JN6tt3A04utN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "C12dETlRQtc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4QZI5rRltTSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176b5242-4906-491f-e334-ab06470d038a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cells below to setup and install the required libraries. For our experiment we will need accelerate, peft, transformers, datasets,scipy and TRL to leverage SFTTrainer. We will use bitsandbytes to quantize the base model into 4bit. We will also install einops but it was mainly used for loading falcon so I will remove it in later versions."
      ],
      "metadata": {
        "id": "HA_UQtIHQmqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops scipy wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95UjoZmlQ44B",
        "outputId": "07db3423-090b-4ce8-e98c-a775668003eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB:\n",
        "- The `bitsandbytes` is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.\n",
        "- `Quantization` is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\n",
        "- `[Accelerate]`(https://huggingface.co/docs/accelerate/index) is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.\n",
        "-`einops` Einops is a Python library that provides a flexible and powerful way to manipulate tensors and perform operations on them. The name stands for \"Einstein Operations\" as its syntax is inspired by Einstein summation conventions.\n",
        "- `scipy`: SciPy provides algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, differential equations, statistics and many other classes of problems.\n",
        "- `wandb`:WandB is a central dashboard to keep track of your hyperparameters, system metrics, and predictions so you can compare models live, and share your findings.\n",
        "\n"
      ],
      "metadata": {
        "id": "F_qv-sOU6JCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "https://huggingface.co/datasets/TokenBender/code_instructions_120k_alpaca_style"
      ],
      "metadata": {
        "id": "KANQ7HKoRP1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = 'TokenBender/code_instructions_120k_alpaca_style'\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBUkE8cNRUMh",
        "outputId": "6bbc99ca-6e17-40dd-c5b3-a6bab2cc03e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/TokenBender___json/TokenBender--code_instructions_120k_alpaca_style-908250f530b0c0f4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "83Vio_DTlgC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340fa9f8-dd8a-4278-8a5b-4d50e0e4ae8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input', 'instruction', 'text', 'output'],\n",
              "    num_rows: 121959\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: This dataset holds several key-value pairs:\n",
        "`input` and `output` holds the query and answer. `instruction` might hold some sort of command or directive related to the data, and `text` might hold some additional textual data."
      ],
      "metadata": {
        "id": "Fqci7cqU-Z41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "G6jCN-zf-BOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba7ed7b-39c4-4b5f-f029-f4be90767dac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '[1, 2, 3, 4, 5]',\n",
              " 'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n",
              " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Create a function to calculate the sum of a sequence of integers. ### Input: [1, 2, 3, 4, 5] ### Output: # Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
              " 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loging to the HF hub to get access to the authentication token\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "f623790c790a4049a47eed0326f53017",
            "c15113fd98a741a5a00ec102b7a209b9",
            "189b3a6fd39c4b339ab981412cdc6728",
            "75e7fbf728f44babb48bfd6085a7871e",
            "1a6b5bff459a4968966a08e88ccfee3c",
            "f7e35898035c4d98ac9de8e4d8b65b4c",
            "5105d20d76d24a409e8454899ab9d1b5",
            "13bf1ed303904d338a60208c372bd75a",
            "6c9fd03b7e8c4437b451e4f83eeeadf3",
            "280b8932e7da4c75b665145ea557f56a",
            "d95e0a5533ac4dd283fe1bfafde53136",
            "1d9a68eb7f2341f7895bc722c8790e45",
            "89a4b0bd2c9b4d8ea443d2b7566645dd",
            "82d9d8d997724cdd803487e10205f847",
            "58493e0814cf48d0afdf96a82d0133e6",
            "969c521253a847468293ca594a35c500",
            "30b1df499d3442b2b97a908894b0af3d",
            "81f1694d26f44d8d96d82f7949d912ad",
            "338b66b17b8f4793b06f266ff31f7cd5",
            "9ed75bed6bce4e7b976ac39e91d93141",
            "0614255379fa49159ac5ded90437b137",
            "c7d57835a51e4cb88d43ee93d0943c5a",
            "5a45ed5433924cbdb07225435af0e312",
            "150aa92ece3a436791164a9df5742594",
            "92bd68aaf4f64ca1ae046d64d020879c",
            "511274491d234b379783f05f70c0bdd8",
            "2bad4db9e9c0405e94291e2bb20094d1",
            "0df7b837a0544ab696855cb093324872",
            "8728f858dc6847729cf536add0e5a628",
            "200651f8c7ff450b8aa9051ece8fd238",
            "499f7147aca544c4996a5ffa01e72ac6",
            "65621765fbc14d73a522f7b6a3365ee4"
          ]
        },
        "id": "0cnQqzY7IcE6",
        "outputId": "d7f25d53-3f9c-4b31-8cc9-7fe5bf3afabe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f623790c790a4049a47eed0326f53017"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
      ],
      "metadata": {
        "id": "AbCErVJuI0ua"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Model"
      ],
      "metadata": {
        "id": "dZq9MvmWP8Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-trained transformer model for causal language modeling\n",
        "# (i.e., predicting the next word in a sentence)\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
        "\n",
        "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "#if you're running on google colab free tier, uncomment below model and use it instead\n",
        "model_name = \"abhishek/llama-2-7b-hf-small-shards\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # The \"nf4\" value suggests that the model is using \"narrow full\" 4-bit quantization, which is a specific scheme of 4-bit quantization that prioritizes retaining more information over saving more memory.\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dc71f1673e0e41e894c1dc67cbb0d793",
            "2e7c02da13de4c4f9f70279fad462a56",
            "f2237504735e4ffe98dbeec509cb951e",
            "bdba23601fd34fbebd5174e6c6050025",
            "5fb5d7e968064273a9bf027001942bb0",
            "928b699cb54a46b1a96db5faeb33f5fe",
            "473b8a657eeb4eb0b3f651ddb34d506d",
            "bd449b5b928e4f9aae06f4a7de5e1aa6",
            "585574ed98e343e58256409e124e9b98",
            "1ee124d26a0842f9b350df2fd4e0d0be",
            "3a3f5abecb514056ae06d1f7d09a0a28"
          ]
        },
        "id": "XYLKq-IWJAyS",
        "outputId": "ac33218c-2df2-4b46-f04a-362d21855da9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc71f1673e0e41e894c1dc67cbb0d793"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also load the tokenizer below"
      ],
      "metadata": {
        "id": "fQj5KJ_6SrB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the tokenizer used for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "MmQnvhbsSuVs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nb:\n",
        "`tokenizer.pad_token = tokenizer.eos_token` is setting the pad token to be the same as the EOS token. This might be done if your model needs to interpret padding as the end of a sentence, or if you're working with a model or dataset that already uses the EOS token for padding.\n",
        "`trust_remote_code=True`: This is a flag used to allow or disallow the execution of custom code in the tokenizer configuration file. If trust_remote_code=True, it means the tokenizer configuration file is allowed to run custom code. This can be useful when the tokenizer includes some special rules or procedures, but it can potentially be a security risk if the source of the tokenizer is not trusted."
      ],
      "metadata": {
        "id": "dLv12LMlm1Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add dense, dense_h_to_4_h and dense_4h_to_h layers in the target modules in addition to the mixed query key value layer."
      ],
      "metadata": {
        "id": "em_a-8qb8B9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA - setting up hyperparameters for LoraConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.03\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "kyHKAreMS3XL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: LoRA hyperparameters:\n",
        "\n",
        "- `lora_alpha=16` ( int , optional) – A hyper-parameter to control the init scale of loralib.\n",
        "- `lora_dropout=0.1` ( float , optional) – The dropout rate in lora.\n",
        "- `lora_r=64` means that the low-rank matrix used in the LoRA method will have a **rank of 64**. The \"rank\" of a matrix is the maximum number of linearly independent rows or columns in the matrix. This is a hyperparameter that can be tuned depending on the specific task and the resources available for training the model.\n",
        "- `target_modules`: \"`dense_h_to_4h`\" and \"`dense_4h_to_h`\": These likely refer to specific dense layers in the model. The names suggest these layers might be involved in projecting the model's hidden state h to a space that's four times larger, and then back down to the original size. **This is a common pattern in transformer models, where the input to the feed-forward network is first projected to a larger dimension (often called the expansion dimension), and then projected back to the original dimension**.\n",
        "\n",
        "By specifying these modules as the `target_modules`, you're telling the LoRA method to specifically target these parts of the model during the fine-tuning process. The exact effect will depend on how the LoRA method is implemented, but generally, it will involve modifying these modules in some way to improve the model's ability to rank and select high-quality answers.\n"
      ],
      "metadata": {
        "id": "6Yj7VqkZpc4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: `peft`: [Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)}"
      ],
      "metadata": {
        "id": "GLVHySg0_R_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Trainer\n",
        "\n",
        "Here we will use the SFTTrainer from TRL library that gives a wrapper around transformers Trainer to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."
      ],
      "metadata": {
        "id": "MHPc3kjTTDts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the training configuration for the model\n",
        "output_dir = \"./results\"\n",
        "per_device_train_batch_size = 4 # batch size per device\n",
        "gradient_accumulation_steps = 4 # the number of steps to accumulate gradients before performing an optimization step\n",
        "optim = \"paged_adamw_32bit\" # AdamW optimizer that works with 32-bit precision.\n",
        "save_steps = 100 # # model checkpoints\n",
        "logging_steps = 10 # model logging\n",
        "learning_rate = 1.4e-4 # learning rate for the optimizer\n",
        "max_grad_norm = 0.3 # sets the maximum norm of the gradients for gradient clipping to avoid exploding gradients\n",
        "max_steps = 200 # number of training steps\n",
        "warmup_ratio = 0.03 # sets the ratio of warmup steps in the learning rate scheduler\n",
        "lr_scheduler_type = \"constant\" # A \"constant\" scheduler keeps the learning rate constant throughout training.\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    #number_train_epoch=1\n",
        ")"
      ],
      "metadata": {
        "id": "UPxb9sUjTYk-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: **Mixed Precision Training**: This is a method used to speed up training and reduce the memory requirements of your model. In mixed precision training, some of your model's parameters and activations are stored as 16-bit floating point numbers (as opposed to the standard 32-bit), which take up less memory and can be processed faster by certain GPUs.\n",
        "\n",
        "`fp16=True`: By setting this parameter to True, you're telling the training script to use mixed precision training. This means that the model will use a mix of 16-bit and 32-bit floating point numbers during training."
      ],
      "metadata": {
        "id": "VKSXx3LBxvoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then finally pass everthing to the trainer"
      ],
      "metadata": {
        "id": "nc_vz4qTTdvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the trainer for fine-tuning\n",
        "from trl import SFTTrainer\n",
        "\n",
        "#max_seq_length = 2048\n",
        "max_seq_length = 1024\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "d70a3648d7894639ba0b6e170ff06abd",
            "4de339982b58421b93010d61a1054adc",
            "009a3863cc954f9aa98958a73b741933",
            "09e4abecf32b4ef28b8098f857061832",
            "6ccba1417f2c4b6588e3057cecfb0524",
            "a781ad2ca940494d8112e51b0316fd6f",
            "caa87935a39a45f4a9b419dd7225c14f",
            "32078580a2bb49589a2047848ff35720",
            "663c979859fb4c1ca34a3b42657a75ce",
            "b10ca489f66647bb993f9f7c9da34acd",
            "ef1669d467b243118ae2f9d3b937e392"
          ]
        },
        "id": "jfzpX9ciThNJ",
        "outputId": "da6462ab-3e0b-4c28-e8d3-631f16d8b558"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/121959 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d70a3648d7894639ba0b6e170ff06abd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ??SFTTrainer"
      ],
      "metadata": {
        "id": "eKemxXcl0ekW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"
      ],
      "metadata": {
        "id": "wwAE7dy_TstW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the data type of certain modules in a PyTorch model\n",
        "# Specifically, it's changing the data type of all modules with \"norm\" in their name to torch.float32.\n",
        "for name, module in trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float32)\n"
      ],
      "metadata": {
        "id": "T1E2dz9iTvng"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "pNLMN1EepDff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f6bd6f-c8bd-4222-e39d-9e4ecc5d5219"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(\n",
              "            in_features=4096, out_features=4096, bias=False\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.03, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "          )\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(\n",
              "            in_features=4096, out_features=4096, bias=False\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.03, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "          )\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explanation**\n",
        "\n",
        "This block of code is providing a textual representation of a transformer model class (**LlamaForCausalLM**) structure. The architecture of this model seems to be specifically designed for Causal Language Modeling (CLM), a task where the model predicts the next token in a sequence given the history of previous tokens.\n",
        "\n",
        "Here's a breakdown of the main components:\n",
        "\n",
        "**LlamaForCausalLM**: This is the top-level class for the model. It's a transformer model specifically designed for causal language modeling.\n",
        "\n",
        "**LlamaModel**: This is the main body of the model, which contains the layers of the transformer model.\n",
        "\n",
        "**Embedding**: This layer is responsible for converting input tokens into vectors of a specific size (in this case, 4096). **The model has a vocabulary size of 32000 and each token is represented as a 4096-dimensional vector**.\n",
        "\n",
        "**ModuleList**: This is a PyTorch container for holding a list of layers. In this case, **it holds 32 LlamaDecoderLayer instances**.\n",
        "\n",
        "**LlamaDecoderLayer**: This is a **single layer of the transformer model**. Each layer includes self-attention mechanism (**LlamaAttention**) and a feed-forward neural network (**LlamaMLP**), along with normalization layers (**LlamaRMSNorm**).\n",
        "\n",
        "**LlamaAttention**: This represents the **self-attention mechanism in the transformer model**. It includes the query, key, and value projections (**q_proj**, **k_proj**, **v_proj**), each of which is a Linear4bit layer, indicating that they are using 4-bit precision for the linear transformations. It also includes **LoRA** (Low-Rank Adapters) components that are used to reduce the computational complexity of the attention mechanism.\n",
        "\n",
        "**LlamaMLP**: This is the **feed-forward neural network** within each transformer layer, which includes linear layers (Linear4bit) and an activation function (**SiLUActivation**).\n",
        "\n",
        "**LlamaRMSNorm**: This is a variant of Layer Normalization, which is a technique used to stabilize the activations in the model and speed up training.\n",
        "\n",
        "**lm_head**: This is the **final layer of the model**, which maps the output of the transformer layers to the vocabulary size, effectively giving a probability distribution over the vocabulary for each token in the output sequence.\n",
        "\n",
        "Note: This model seems to be using 4-bit precision for its linear layers (Linear4bit), which is likely a part of a quantization strategy to reduce memory usage and speed up computation. It's also using a technique called Low-Rank Adapters (LoRA) to reduce the computational complexity of the attention mechanism. The exact details of these techniques might vary depending on the specific implementation and configuration of the model."
      ],
      "metadata": {
        "id": "nPtq-rguTWXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "Now let's train the model! Simply call `trainer.train()`\n",
        "\n",
        "The `trainer.train()` function call is part of the Hugging Face's Transformers library. This line of code is used to start the training process of a model. When you create a Trainer object in the Transformers library, you typically provide it with:\n",
        "- a model to train,\n",
        "- a training dataset,\n",
        "- a tokenizer, and various training parameters (like the learning rate, batch size, etc.).\n",
        "\n",
        "The Trainer object encapsulates the training loop and provides several utility functions to make training easier."
      ],
      "metadata": {
        "id": "_q7BMInopKnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "nZfaA95lUAsZ",
        "outputId": "dcd73135-b5e0-4cc4-df03-447a477b0084"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mroyam0820\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230724_122025-vrjzzg44</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/royam0820/huggingface/runs/vrjzzg44' target=\"_blank\">balmy-glade-7</a></strong> to <a href='https://wandb.ai/royam0820/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/royam0820/huggingface' target=\"_blank\">https://wandb.ai/royam0820/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/royam0820/huggingface/runs/vrjzzg44' target=\"_blank\">https://wandb.ai/royam0820/huggingface/runs/vrjzzg44</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 13:01, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>61.378600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.417900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>9.836800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.235400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>40.775100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>11.101500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.483200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>40.874500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>13.237400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>13.415600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>36.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>13.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>24.519700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>55.721400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>27.390400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>31.051400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>82.310300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.474800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>19.628600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>74.123700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=28.307640545368194, metrics={'train_runtime': 801.655, 'train_samples_per_second': 3.992, 'train_steps_per_second': 0.249, 'total_flos': 1.3095702503424e+16, 'train_loss': 28.307640545368194, 'epoch': 0.03})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Results Explanation**\n",
        "\n",
        "**global_step=200**: This indicates that the model was trained for 200 steps. A step usually means one update to the model's weights, which typically occurs after processing a batch of data.\n",
        "\n",
        "**training_loss**=28.307640545368194: This is the loss value of the model on the training data. The loss is a measure of how well the model's predictions match the actual values. Lower loss values indicate that the model's predictions are closer to the actual values.\n",
        "\n",
        "**metrics**: This is a dictionary containing various metrics that provide additional information about the training process:\n",
        "\n",
        "> **train_runtime**: 801.655 This indicates that the training process took approximately 801.655 seconds.\n",
        "\n",
        "> **train_samples_per_second**: 3.992 This measures the speed of the training process in terms of the number of training samples processed per second.\n",
        "\n",
        "> **train_steps_per_second**: 0.249 This measures the speed of the training process in terms of the number of training steps performed per second.\n",
        "\n",
        "> **total_flos**: 1.3095702503424e+16 FLOPS (Floating Point Operations Per Second) is a measure of computer performance, useful in fields of scientific computations that require floating-point calculations. This is the total number of floating point operations that were performed during training.\n",
        "\n",
        "> **train_loss**: 28.307640545368194 This is the same as the training_loss mentioned above. (13.36 mn)\n",
        "\n",
        "> **epoch**: 0.03 This indicates that the training process completed 0.03 epochs. An epoch is one complete pass through the entire training dataset. In this case, it means the training process did not complete a full pass through the training data.\n",
        "\n",
        "NB: the epoch value of \"0.03\" likely means that the training process only went through 3% of your entire training dataset. This could happen if you're training for a fixed number of steps that doesn't cover the whole dataset, or if you stop training early.\n",
        "Indeed, the number of epochs can be a fraction if training doesn't complete a full pass through the data. For example, if you have a training dataset of 1000 examples, and you set up your training loop to run for 200 steps with a batch size of 10, then you would only go through 200*10 = 2000 examples during training. Since this is two times the size of your dataset, you've effectively gone through \"2 epochs\".\n",
        "\n"
      ],
      "metadata": {
        "id": "JgfKAgNMYXlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During QLoRA training, the training losses are spiking and falling sharply. The training loss also drops to zero after 200 steps in my training.\n",
        "\n",
        "The SFTTrainer will take care of properly saving only the adapters during training instead of saving the entire model."
      ],
      "metadata": {
        "id": "PqQHSS5iUSn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputs\")"
      ],
      "metadata": {
        "id": "2KltfwVTUcon"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inference"
      ],
      "metadata": {
        "id": "IgSOJm-JNc3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "FItXaYoe-FSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca164467-7a16-412a-c68b-1e92cf775fd5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 24 12:35:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    41W / 300W |  12584MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig.from_pretrained('outputs')\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "6hPdVNDJV0GB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`lora_config = LoraConfig.from_pretrained('outputs')`: This line is loading a pre-trained LoraConfig from the directory named 'outputs'. LoraConfig likely contains the configuration parameters for the LoRA technique, such as the rank of the approximation, dropout rate, and other parameters.\n",
        "\n",
        "`model = get_peft_model(model, lora_config)`: This line is calling a function named get_peft_model to apply the LoRA technique to the model. It's taking two arguments: the original model and the LoRA configuration. The function is expected to return a new model that has been modified according to the LoRA technique as specified by the lora_config.\n",
        "\n",
        "LoRA is a technique used to reduce the parameter count and computational complexity of large models. It does this by approximating the large weight matrices in the model with the product of two smaller matrices.\n",
        "\n"
      ],
      "metadata": {
        "id": "a9IXjQ1xbw-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: Lora: is a technique that accelerates the fine-tuning of large models while consuming less memory.`lora_config` allows you to control how LoRA is applied to the base model through the following parameters: r : the rank of the update matrices, expressed in int ."
      ],
      "metadata": {
        "id": "pPFYo8VZAObZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt formatting\n",
        "text = '''[INST]<<SYS>>\n",
        " You are a helpful coding assistant that provides code based on the given query in context.\n",
        "<</SYS>>\n",
        "Write a python program to perform binary search in a given list.[/INST]'''\n",
        "device = \"cuda:0\""
      ],
      "metadata": {
        "id": "h-PcUNFcV4Q-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`[INST]`: marker indicating the start of an instruction.\n",
        "\n",
        "`<<SYS>>`: marker indicating the start of a system message. System messages might contain metadata or additional instructions for the AI model.\n",
        "\n",
        "*You are a helpful coding assistant that provides code based on the given query in context.* This is the content of the system message.\n",
        "\n",
        "`<</SYS>>`: marker indicating the end of a system message.\n",
        "\n",
        "*Write a python program to perform binary search in a given list.: This is the actual task that the AI model is being asked to complete.*\n",
        "\n",
        "[/INST]: marker indicating the end of an instruction.`"
      ],
      "metadata": {
        "id": "5Hicy4Kac1WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code indicates the following:\n",
        "\n",
        "`inputs = tokenizer(text, return_tensors=\"pt\").to(device)`: The input text is tokenized, and return_tensors=\"pt\" means that the tokenizer should return PyTorch tensors. The .to(device) part sends the inputs to the specified device, which is generally a GPU (\"cuda:0\") or CPU (\"cpu\").\n",
        "\n",
        "`outputs = model.generate(**inputs, max_new_tokens=1024)`: The model generates output text given the input. It does this by predicting what comes next for a maximum of 1024 tokens. The **inputs is using Python's syntax for passing the key-value pairs in the dictionary inputs as keyword arguments to the generate function.\n",
        "\n",
        "`print(tokenizer.decode(outputs[0], skip_special_tokens=True))`: The generated output, which is in the form of token IDs, is decoded back into readable text. skip_special_tokens=True means that special tokens (like padding or end-of-sequence tokens) used by the model will be removed from the output."
      ],
      "metadata": {
        "id": "nXUL_DL2eBGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxS4JxK3V9Bd",
        "outputId": "e14bb601-7db9-4460-8a9f-c19a71646351"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            " You are a helpful coding assistant that provides code based on the given query in context.\n",
            "<</SYS>>\n",
            "Write a python program to perform binary search in a given list.[/INST]\n",
            "\n",
            "### Solution\n",
            "\n",
            "```python\n",
            "def binary_search(arr, target):\n",
            "    low = 0\n",
            "    high = len(arr) - 1\n",
            "    while low <= high:\n",
            "        mid = (low + high) // 2\n",
            "        if arr[mid] == target:\n",
            "            return mid\n",
            "        elif arr[mid] < target:\n",
            "            high = mid - 1\n",
            "        else:\n",
            "            low = mid + 1\n",
            "    return -1\n",
            "\n",
            "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "target = 5\n",
            "\n",
            "print(binary_search(arr, target))\n",
            "```\n",
            "\n",
            "### Explanation\n",
            "\n",
            "The idea is to find the index of the element in the list that is equal to the given target.\n",
            "\n",
            "The algorithm is as follows:\n",
            "\n",
            "1. Initialize the low and high index to 0 and len(arr) - 1 respectively.\n",
            "2. While the low index is less than the high index, do the following:\n",
            "    - Calculate the mid index by dividing the low and high index by 2.\n",
            "    - If the element at the mid index is equal to the target, return the mid index.\n",
            "    - If the element at the mid index is less than the target, set the low index to the mid index.\n",
            "    - If the element at the mid index is greater than the target, set the high index to the mid index.\n",
            "\n",
            "### References\n",
            "\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3schools.com/python/python_intro.asp)\n",
            "- [Python Tutorial](https://www.w3sch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model push to the HF Hub"
      ],
      "metadata": {
        "id": "KvgUTZigaoYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"amr_llama2-CodeInstr-finetuned-model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "0dcf35c032c646a2a6159994d33bf49a",
            "d085de7c054540b5a676636a00c30053",
            "07f7bb6cb2f34118a3d7505a53fd30f0",
            "276f8da8d2a84e8c8cae348bd17cf721",
            "f303331bb02947dd890abf85610b8ce2",
            "dc4b16b362a14d03bf8737a22f091798",
            "59ad509c976a46e190e8ce8bb0e0f4cf",
            "b119483431a0469194bbde5d41fcba5d",
            "42fd3f5d88f84de987e03455091da066",
            "b78952b368474eec878a0eb6ba8f2e59",
            "9effa02eda9b4a5bbeb600b83d02561c"
          ]
        },
        "id": "mpTChGbAWHws",
        "outputId": "8e6fc6c7-d0be-4e35-e85e-199ce0d83846"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dcf35c032c646a2a6159994d33bf49a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/royam0820/amr_llama2-CodeInstr-finetuned-model/commit/fbde88ba3b703f1191834697238d6b5a6dc27653', commit_message='Upload model', commit_description='', oid='fbde88ba3b703f1191834697238d6b5a6dc27653', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Web UI for Llama"
      ],
      "metadata": {
        "id": "h7Nzpa2DB1Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "GOVEBSTad52T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "bEzBgArkdxG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee65fd1-3010-484c-ebed-612c7e0c106b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.10 (from gradio)\n",
            "  Downloading gradio_client-0.2.10-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.11)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.7.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.10->gradio) (2023.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=b012f3083140ac67d14c12ec83efa2104046eec7b864f3b024c26e8d52f6ca77\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.100.0 ffmpy-0.3.1 gradio-3.38.0 gradio-client-0.2.10 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uc-micro-py-1.0.2 uvicorn-0.23.1 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will import:\n",
        "- `gradio` is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!\n",
        "- `text_generation` is an llm task for producing new text. These models can, for example, fill in incomplete text or paraphrase."
      ],
      "metadata": {
        "id": "Ggho10DAolxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text_generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNnt19BAGXzU",
        "outputId": "1ff81ed0-8747-448a-cd96-f814d622f8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting text_generation\n",
            "  Downloading text_generation-0.6.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.8 in /usr/local/lib/python3.10/dist-packages (from text_generation) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from text_generation) (0.16.4)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from text_generation) (1.10.11)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.8->text_generation) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.8->text_generation) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.8->text_generation) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.8->text_generation) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.8->text_generation) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.8->text_generation) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.8->text_generation) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.12->text_generation) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.12->text_generation) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.12->text_generation) (23.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.8->text_generation) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (2023.5.7)\n",
            "Installing collected packages: text_generation\n",
            "Successfully installed text_generation-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: Text generation works by utilizing algorithms and language models to process input data and generate output text. It involves training AI models on large datasets of text to learn patterns, grammar, and contextual information. These models then use this learned knowledge to generate new text based on given prompts or conditions."
      ],
      "metadata": {
        "id": "RZV4zj7hLpzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import gradio as gr\n",
        "from text_generation import Client\n",
        "\n",
        "\n",
        "# llama prompt starts with <s> and ends with </s>\n",
        "# system prompt\n",
        "PROMPT = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Specify the URL of the LLM server\n",
        "\n",
        "#LLAMA_70B = os.environ.get(\"LLAMA_70B\", \"http://localhost:3000\")\n",
        "LLAMA_70B = os.environ.get(\"https://huggingface.co/royam0820/llama2-chat-hub-my-finetuned-model\")\n",
        "CLIENT = Client(base_url=LLAMA_70B)\n",
        "\n",
        "# creating a dictionary for the LLM parameters\n",
        "PARAMETERS = {\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_p\": 0.95,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"top_k\": 50,\n",
        "    \"truncate\": 1000,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"seed\": 42,\n",
        "    \"stop_sequences\": [\"</s>\"],\n",
        "}\n",
        "\n",
        "# Message formatting\n",
        "def format_message(message, history, memory_limit=5):\n",
        "    # handling the context, keeping 5 last messages in memory\n",
        "    # always keep len(history) <= memory_limit\n",
        "    if len(history) > memory_limit:\n",
        "        history = history[-memory_limit:]\n",
        "\n",
        "    if len(history) == 0:\n",
        "        return PROMPT + f\"{message} [/INST]\"\n",
        "\n",
        "    formatted_message = PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n",
        "\n",
        "    # Handle conversation history\n",
        "    for user_msg, model_answer in history[1:]:\n",
        "        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n",
        "\n",
        "    # Handle the current message\n",
        "    formatted_message += f\"<s>[INST] {message} [/INST]\"\n",
        "\n",
        "    return formatted_message\n",
        "\n",
        "# Inference\n",
        "# message is the current user query, history are the past user's queries\n",
        "def predict(message, history):\n",
        "    query = format_message(message, history)\n",
        "    text = \"\" #it will hold the response text\n",
        "    for response in CLIENT.generate_stream(query, **PARAMETERS):\n",
        "        if not response.token.special:\n",
        "            text += response.token.text\n",
        "            yield text\n",
        "\n",
        "# lauching the Gradio webui chat interface\n",
        "gr.ChatInterface(predict).queue().launch()"
      ],
      "metadata": {
        "id": "0qVZqJhXC1_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "0dbd7c0d-09b5-4e88-a012-edad67a65cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ff6174b69466bae4a1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ff6174b69466bae4a1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}